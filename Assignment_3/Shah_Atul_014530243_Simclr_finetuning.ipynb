{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shah_Atul_014530243_Simclr_finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d227e00d1d3342b2967383155bcdea99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_838fb0f148864854978b83f56bd34b9a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cdc80abe176444c6851a03fe1174b752",
              "IPY_MODEL_73d4f9a713b545f9a239ebdd8a9c1c72"
            ]
          }
        },
        "838fb0f148864854978b83f56bd34b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdc80abe176444c6851a03fe1174b752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3b56759da43243d993fc5d4a96af0a31",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_845236952ae144caa818392e1f52093d"
          }
        },
        "73d4f9a713b545f9a239ebdd8a9c1c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6645f99daf7447afbaaad4095f5846c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:04&lt;00:00,  1.10 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa3f9706f0db48e3a212472a841fcff3"
          }
        },
        "3b56759da43243d993fc5d4a96af0a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "845236952ae144caa818392e1f52093d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6645f99daf7447afbaaad4095f5846c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa3f9706f0db48e3a212472a841fcff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atulshah16/EmergingTechnologiesCMPE297/blob/master/Assignment_3/Shah_Atul_014530243_Simclr_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB0hnoeH2HoU"
      },
      "source": [
        "### Name: Atul Shah\n",
        "### Student ID: 014530243\n",
        "\n",
        "# Simclr Finetuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXLDCdO4wLOV"
      },
      "source": [
        "# Resnet50 pretrained model.\n",
        "\n",
        "### First we show a pretained RESNET50 model on flower dataset without finetuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SftIf28fbGJq"
      },
      "source": [
        "FLOWERS_DIR = './flower_photos'\n",
        "TRAIN_FRACTION = 0.8\n",
        "RANDOM_SEED = 2018\n",
        "\n",
        "\n",
        "def download_images():\n",
        "  \n",
        "  if not os.path.exists(FLOWERS_DIR):\n",
        "    DOWNLOAD_URL = 'http://download.tensorflow.org/example_images/flower_photos.tgz'\n",
        "    print('Downloading flower images from %s...' % DOWNLOAD_URL)\n",
        "    urllib.request.urlretrieve(DOWNLOAD_URL, 'flower_photos.tgz')\n",
        "    !tar xfz flower_photos.tgz\n",
        "  print('Flower photos are located in %s' % FLOWERS_DIR)\n",
        "\n",
        "\n",
        "def make_train_and_test_sets():\n",
        "  \n",
        "  train_examples, test_examples = [], []\n",
        "  shuffler = random.Random(RANDOM_SEED)\n",
        "  is_root = True\n",
        "  for (dirname, subdirs, filenames) in tf.gfile.Walk(FLOWERS_DIR):\n",
        " \n",
        "    if is_root:\n",
        "      subdirs = sorted(subdirs)\n",
        "      classes = collections.OrderedDict(enumerate(subdirs))\n",
        "      label_to_class = dict([(x, i) for i, x in enumerate(subdirs)])\n",
        "      is_root = False\n",
        "    \n",
        "    else:\n",
        "      filenames.sort()\n",
        "      shuffler.shuffle(filenames)\n",
        "      full_filenames = [os.path.join(dirname, f) for f in filenames]\n",
        "      label = dirname.split('/')[-1]\n",
        "      label_class = label_to_class[label]\n",
        "     \n",
        "      examples = list(zip(full_filenames, [label_class] * len(filenames)))\n",
        "      num_train = int(len(filenames) * TRAIN_FRACTION)\n",
        "      train_examples.extend(examples[:num_train])\n",
        "      test_examples.extend(examples[num_train:])\n",
        "\n",
        "  shuffler.shuffle(train_examples)\n",
        "  shuffler.shuffle(test_examples)\n",
        "  return train_examples, test_examples, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7JVzjY9bImR",
        "outputId": "a1a21534-e963-40c0-9918-745beb2465e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "download_images()\n",
        "TRAIN_EXAMPLES, TEST_EXAMPLES, CLASSES = make_train_and_test_sets()\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "print('\\nThe dataset has %d label classes: %s' % (NUM_CLASSES, CLASSES.values()))\n",
        "print('There are %d training images' % len(TRAIN_EXAMPLES))\n",
        "print('there are %d test images' % len(TEST_EXAMPLES))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flower images from http://download.tensorflow.org/example_images/flower_photos.tgz...\n",
            "Flower photos are located in ./flower_photos\n",
            "\n",
            "The dataset has 5 label classes: odict_values(['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'])\n",
            "There are 2934 training images\n",
            "there are 736 test images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JqLf67FbIjt",
        "outputId": "8798f90d-be53-4f11-c51a-6db2e14a1a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "LEARNING_RATE = 0.01\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "image_module = hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\")\n",
        "\n",
        "\n",
        "encoded_images = tf.placeholder(tf.string, shape=[None])\n",
        "\n",
        "image_size = (128, 128)\n",
        "print(image_size)\n",
        "\n",
        "def decode_and_resize_image(encoded):\n",
        "  decoded = tf.image.decode_jpeg(encoded, channels=3)\n",
        "  decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
        "  return tf.image.resize_images(decoded, image_size)\n",
        "\n",
        "\n",
        "batch_images = tf.map_fn(decode_and_resize_image, encoded_images, dtype=tf.float32)\n",
        "\n",
        "\n",
        "features = image_module(batch_images)\n",
        "\n",
        "\n",
        "def create_model(features):\n",
        "  \"\"\"Build a model for classification from extracted features.\"\"\"\n",
        "  \n",
        "  layer = tf.layers.dense(inputs=features, units=NUM_CLASSES, activation=None)\n",
        "  return layer\n",
        "\n",
        "\n",
        "\n",
        "logits = create_model(features)\n",
        "labels = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
        "\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels)\n",
        "cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
        "train_op = optimizer.minimize(loss=cross_entropy_mean)\n",
        "\n",
        "\n",
        "probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "prediction = tf.argmax(probabilities, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(labels, 1))\n",
        "\n",
        " \n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 128)\n",
            "WARNING:tensorflow:From <ipython-input-15-6e4ec46db9cd>:24: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-6e4ec46db9cd>:24: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOX_ebegEZS"
      },
      "source": [
        "def get_label(example):\n",
        " \n",
        "  return example[1]\n",
        "\n",
        "def get_class(example):\n",
        "\n",
        "  return CLASSES[get_label(example)]\n",
        "\n",
        "def get_encoded_image(example):\n",
        " \n",
        "  image_path = example[0]\n",
        "  return tf.gfile.GFile(image_path, 'rb').read()\n",
        "\n",
        "def get_image(example):\n",
        "  \n",
        "  return plt.imread(io.BytesIO(get_encoded_image(example)), format='jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_XxLbShbGEz",
        "outputId": "a8cf4493-bbc7-4826-8374-ba86d4791e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "\n",
        "NUM_TRAIN_STEPS = 100 \n",
        "\n",
        "TRAIN_BATCH_SIZE = 10 \n",
        "\n",
        "EVAL_EVERY = 10 \n",
        "\n",
        "def get_batch(batch_size=None, test=False):\n",
        " \n",
        "  examples = TEST_EXAMPLES if test else TRAIN_EXAMPLES\n",
        "  batch_examples = random.sample(examples, batch_size) if batch_size else examples\n",
        "  return batch_examples\n",
        "\n",
        "def get_images_and_labels(batch_examples):\n",
        "  images = [get_encoded_image(e) for e in batch_examples]\n",
        "  one_hot_labels = [get_label_one_hot(e) for e in batch_examples]\n",
        "  return images, one_hot_labels\n",
        "\n",
        "def get_label_one_hot(example):\n",
        "  \n",
        "  one_hot_vector = np.zeros(NUM_CLASSES)\n",
        "  np.put(one_hot_vector, get_label(example), 1)\n",
        "  return one_hot_vector\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(NUM_TRAIN_STEPS):\n",
        "   \n",
        "    train_batch = get_batch(batch_size=TRAIN_BATCH_SIZE)\n",
        "    batch_images, batch_labels = get_images_and_labels(train_batch)\n",
        "  \n",
        "    train_loss, _, train_accuracy = sess.run(\n",
        "        [cross_entropy_mean, train_op, accuracy],\n",
        "        feed_dict={encoded_images: batch_images, labels: batch_labels})\n",
        "    is_final_step = (i == (NUM_TRAIN_STEPS - 1))\n",
        "    if i % EVAL_EVERY == 0 or is_final_step:\n",
        "     \n",
        "      test_batch = get_batch(batch_size=None, test=True)\n",
        "      batch_images, batch_labels = get_images_and_labels(test_batch)\n",
        "   \n",
        "      test_loss, test_accuracy, test_prediction, correct_predicate = sess.run(\n",
        "        [cross_entropy_mean, accuracy, prediction, correct_prediction],\n",
        "        feed_dict={encoded_images: batch_images, labels: batch_labels})\n",
        "      print('Test accuracy at step %s: %.2f%%' % (i, (test_accuracy * 100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy at step 0: 29.35%\n",
            "Test accuracy at step 10: 63.72%\n",
            "Test accuracy at step 20: 60.05%\n",
            "Test accuracy at step 30: 69.57%\n",
            "Test accuracy at step 40: 73.78%\n",
            "Test accuracy at step 50: 74.05%\n",
            "Test accuracy at step 60: 67.53%\n",
            "Test accuracy at step 70: 74.46%\n",
            "Test accuracy at step 80: 75.00%\n",
            "Test accuracy at step 90: 71.60%\n",
            "Test accuracy at step 99: 74.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv8NIBs4_OO4"
      },
      "source": [
        "## With Resnet50 pretrained model, accuracy on flower dataset is 74%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzZZA_VvSBQ"
      },
      "source": [
        "# Simclrv fine tuning\n",
        "\n",
        "### Now we show with simclr finetuning the accuracy becomes much better.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jrMTulRFo_t"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1GfO1RqbUNJ",
        "outputId": "77ffef20-04e5-4f10-8345-d37a8ac381bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import collections\n",
        "import io\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from six.moves import urllib\n",
        "\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sk_metrics\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-WikBQGEJA"
      },
      "source": [
        "EETA_DEFAULT = 0.001\n",
        "\n",
        "class LARSOptimizer(tf.train.Optimizer):\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               momentum=0.9,\n",
        "               use_nesterov=False,\n",
        "               weight_decay=0.0,\n",
        "               exclude_from_weight_decay=None,\n",
        "               exclude_from_layer_adaptation=None,\n",
        "               classic_momentum=True,\n",
        "               eeta=EETA_DEFAULT,\n",
        "               name=\"LARSOptimizer\"):\n",
        "   \n",
        "    super(LARSOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum\n",
        "    self.weight_decay = weight_decay\n",
        "    self.use_nesterov = use_nesterov\n",
        "    self.classic_momentum = classic_momentum\n",
        "    self.eeta = eeta\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "    \n",
        "    if exclude_from_layer_adaptation:\n",
        "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "    else:\n",
        "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    if global_step is None:\n",
        "      global_step = tf.train.get_or_create_global_step()\n",
        "    new_global_step = global_step + 1\n",
        "\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = param.op.name\n",
        "\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/Momentum\",\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      if self._use_weight_decay(param_name):\n",
        "        grad += self.weight_decay * param\n",
        "\n",
        "      if self.classic_momentum:\n",
        "        trust_ratio = 1.0\n",
        "        if self._do_layer_adaptation(param_name):\n",
        "          w_norm = tf.norm(param, ord=2)\n",
        "          g_norm = tf.norm(grad, ord=2)\n",
        "          trust_ratio = tf.where(\n",
        "              tf.greater(w_norm, 0), tf.where(\n",
        "                  tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm),\n",
        "                  1.0),\n",
        "              1.0)\n",
        "        scaled_lr = self.learning_rate * trust_ratio\n",
        "\n",
        "        next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
        "        if self.use_nesterov:\n",
        "          update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
        "        else:\n",
        "          update = next_v\n",
        "        next_param = param - update\n",
        "      else:\n",
        "        next_v = tf.multiply(self.momentum, v) + grad\n",
        "        if self.use_nesterov:\n",
        "          update = tf.multiply(self.momentum, next_v) + grad\n",
        "        else:\n",
        "          update = next_v\n",
        "\n",
        "        trust_ratio = 1.0\n",
        "        if self._do_layer_adaptation(param_name):\n",
        "          w_norm = tf.norm(param, ord=2)\n",
        "          v_norm = tf.norm(update, ord=2)\n",
        "          trust_ratio = tf.where(\n",
        "              tf.greater(w_norm, 0), tf.where(\n",
        "                  tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm),\n",
        "                  1.0),\n",
        "              1.0)\n",
        "        scaled_lr = trust_ratio * self.learning_rate\n",
        "        next_param = param - scaled_lr * update\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           v.assign(next_v),\n",
        "           global_step.assign(new_global_step)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _use_weight_decay(self, param_name):\n",
        "    \n",
        "    if not self.weight_decay:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _do_layer_adaptation(self, param_name):\n",
        "   \n",
        "    if self.exclude_from_layer_adaptation:\n",
        "      for r in self.exclude_from_layer_adaptation:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-v2AIeWBFc5"
      },
      "source": [
        "## Adding noise to image dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDAKcbMTLtzT"
      },
      "source": [
        "FLAGS_color_jitter_strength = 0.3\n",
        "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
        "\n",
        "\n",
        "def random_apply(func, p, x):\n",
        "    return tf.cond(\n",
        "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "              tf.cast(p, tf.float32)),\n",
        "      lambda: func(x),\n",
        "      lambda: x)\n",
        "\n",
        "\n",
        "def random_brightness(image, max_delta, impl='simclrv2'):\n",
        "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
        "  if impl == 'simclrv2':\n",
        "    factor = tf.random_uniform(\n",
        "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
        "    image = image * factor\n",
        "  elif impl == 'simclrv1':\n",
        "    image = random_brightness(image, max_delta=max_delta)\n",
        "  else:\n",
        "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
        "  return image\n",
        "\n",
        "\n",
        "def to_grayscale(image, keep_channels=True):\n",
        "  image = tf.image.rgb_to_grayscale(image)\n",
        "  if keep_channels:\n",
        "    image = tf.tile(image, [1, 1, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "def color_jitter(image,\n",
        "                 strength,\n",
        "                 random_order=True):\n",
        "  brightness = 0.8 * strength\n",
        "  contrast = 0.8 * strength\n",
        "  saturation = 0.8 * strength\n",
        "  hue = 0.2 * strength\n",
        "  if random_order:\n",
        "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
        "  else:\n",
        "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
        "\n",
        "\n",
        "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      if brightness != 0 and i == 0:\n",
        "        x = random_brightness(x, max_delta=brightness)\n",
        "      elif contrast != 0 and i == 1:\n",
        "        x = tf.image.random_contrast(\n",
        "            x, lower=1-contrast, upper=1+contrast)\n",
        "      elif saturation != 0 and i == 2:\n",
        "        x = tf.image.random_saturation(\n",
        "            x, lower=1-saturation, upper=1+saturation)\n",
        "      elif hue != 0:\n",
        "        x = tf.image.random_hue(x, max_delta=hue)\n",
        "      return x\n",
        "\n",
        "    for i in range(4):\n",
        "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      def brightness_foo():\n",
        "        if brightness == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return random_brightness(x, max_delta=brightness)\n",
        "      def contrast_foo():\n",
        "        if contrast == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
        "      def saturation_foo():\n",
        "        if saturation == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_saturation(\n",
        "              x, lower=1-saturation, upper=1+saturation)\n",
        "      def hue_foo():\n",
        "        if hue == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_hue(x, max_delta=hue)\n",
        "      x = tf.cond(tf.less(i, 2),\n",
        "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
        "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
        "      return x\n",
        "\n",
        "    perm = tf.random_shuffle(tf.range(4))\n",
        "    for i in range(4):\n",
        "      image = apply_transform(perm[i], image)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def _compute_crop_shape(\n",
        "    image_height, image_width, aspect_ratio, crop_proportion):\n",
        "  image_width_float = tf.cast(image_width, tf.float32)\n",
        "  image_height_float = tf.cast(image_height, tf.float32)\n",
        "\n",
        "  def _requested_aspect_ratio_wider_than_image():\n",
        "    crop_height = tf.cast(tf.rint(\n",
        "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.rint(\n",
        "        crop_proportion * image_width_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  def _image_wider_than_requested_aspect_ratio():\n",
        "    crop_height = tf.cast(\n",
        "        tf.rint(crop_proportion * image_height_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.rint(\n",
        "        crop_proportion * aspect_ratio *\n",
        "        image_height_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  return tf.cond(\n",
        "      aspect_ratio > image_width_float / image_height_float,\n",
        "      _requested_aspect_ratio_wider_than_image,\n",
        "      _image_wider_than_requested_aspect_ratio)\n",
        "\n",
        "\n",
        "def center_crop(image, height, width, crop_proportion):\n",
        "  shape = tf.shape(image)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "  crop_height, crop_width = _compute_crop_shape(\n",
        "      image_height, image_width, height / width, crop_proportion)\n",
        "  offset_height = ((image_height - crop_height) + 1) // 2\n",
        "  offset_width = ((image_width - crop_width) + 1) // 2\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "  image = tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
        "    shape = tf.shape(image)\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        shape,\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
        "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image, offset_y, offset_x, target_height, target_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def crop_and_resize(image, height, width):\n",
        "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
        "  Args:\n",
        "    image: Tensor representing the image.\n",
        "    height: Desired image height.\n",
        "    width: Desired image width.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
        "  \"\"\"\n",
        "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
        "  aspect_ratio = width / height\n",
        "  image = distorted_bounding_box_crop(\n",
        "      image,\n",
        "      bbox,\n",
        "      min_object_covered=0.1,\n",
        "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
        "      area_range=(0.08, 1.0),\n",
        "      max_attempts=100,\n",
        "      scope=None)\n",
        "  return tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "\n",
        "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
        "  \"\"\"Blurs the given image with separable convolution.\n",
        "  Args:\n",
        "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
        "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
        "      be an odd number. If it is an even number, the actual kernel size will be\n",
        "      size + 1.\n",
        "    sigma: Sigma value for gaussian operator.\n",
        "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
        "  Returns:\n",
        "    A Tensor representing the blurred image.\n",
        "  \"\"\"\n",
        "  radius = tf.to_int32(kernel_size / 2)\n",
        "  kernel_size = radius * 2 + 1\n",
        "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
        "  blur_filter = tf.exp(\n",
        "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
        "  blur_filter /= tf.reduce_sum(blur_filter)\n",
        "  # One vertical and one horizontal filter.\n",
        "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
        "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
        "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
        "  expand_batch_dim = image.shape.ndims == 3\n",
        "  if expand_batch_dim:\n",
        "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
        "    # an extra dimension.\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
        "  if expand_batch_dim:\n",
        "    blurred = tf.squeeze(blurred, axis=0)\n",
        "  return blurred\n",
        "\n",
        "\n",
        "def random_crop_with_resize(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly crop and resize an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: Probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  def _transform(image):  # pylint: disable=missing-docstring\n",
        "    image = crop_and_resize(image, height, width)\n",
        "    return image\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_color_jitter(image, p=1.0):\n",
        "  def _transform(image):\n",
        "    color_jitter_t = functools.partial(\n",
        "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
        "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
        "    return random_apply(to_grayscale, p=0.2, x=image)\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_blur(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly blur an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  del width\n",
        "  def _transform(image):\n",
        "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
        "    return gaussian_blur(\n",
        "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
        "  \"\"\"Apply efficient batch data transformations.\n",
        "  Args:\n",
        "    images_list: a list of image tensors.\n",
        "    height: the height of image.\n",
        "    width: the width of image.\n",
        "    blur_probability: the probaility to apply the blur operator.\n",
        "  Returns:\n",
        "    Preprocessed feature list.\n",
        "  \"\"\"\n",
        "  def generate_selector(p, bsz):\n",
        "    shape = [bsz, 1, 1, 1]\n",
        "    selector = tf.cast(\n",
        "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
        "        tf.float32)\n",
        "    return selector\n",
        "\n",
        "  new_images_list = []\n",
        "  for images in images_list:\n",
        "    images_new = random_blur(images, height, width, p=1.)\n",
        "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
        "    images = images_new * selector + images * (1 - selector)\n",
        "    images = tf.clip_by_value(images, 0., 1.)\n",
        "    new_images_list.append(images)\n",
        "\n",
        "  return new_images_list\n",
        "\n",
        "\n",
        "def preprocess_for_train(image, height, width,\n",
        "                         color_distort=True, crop=True, flip=True):\n",
        "  if crop:\n",
        "    image = random_crop_with_resize(image, height, width)\n",
        "  if flip:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "  if color_distort:\n",
        "    image = random_color_jitter(image)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width, crop=True):\n",
        "  \n",
        "  if crop:\n",
        "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width, is_training=False,\n",
        "                     color_distort=True, test_crop=True):\n",
        "  \n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width, color_distort)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width, test_crop)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV0iIT7DGMie",
        "outputId": "92607478-fea7-4799-8a47-88f647cf0802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "d227e00d1d3342b2967383155bcdea99",
            "838fb0f148864854978b83f56bd34b9a",
            "cdc80abe176444c6851a03fe1174b752",
            "73d4f9a713b545f9a239ebdd8a9c1c72",
            "3b56759da43243d993fc5d4a96af0a31",
            "845236952ae144caa818392e1f52093d",
            "6645f99daf7447afbaaad4095f5846c2",
            "fa3f9706f0db48e3a212472a841fcff3"
          ]
        }
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 64\n",
        "dataset_name = 'tf_flowers'\n",
        "\n",
        "tfds_dataset, tfds_info = tfds.load(\n",
        "    dataset_name, split='train', with_info=True)\n",
        "num_images = tfds_info.splits['train'].num_examples\n",
        "num_classes = tfds_info.features['label'].num_classes\n",
        "\n",
        "def _preprocess(x):\n",
        "  x['image'] = preprocess_image(\n",
        "      x['image'], 224, 224, is_training=False, color_distort=False)\n",
        "  return x\n",
        "x = tfds_dataset.map(_preprocess).batch(batch_size)\n",
        "x = tf.data.make_one_shot_iterator(x).get_next()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset tf_flowers/3.0.0 (download: 218.21 MiB, generated: Unknown size, total: 218.21 MiB) to /root/tensorflow_datasets/tf_flowers/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d227e00d1d3342b2967383155bcdea99",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=5.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cchFKtx9L5mZ",
        "outputId": "68d264bf-c0bc-402a-d3bf-0f685edb2ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "weight_decay = 0.\n",
        "\n",
        "\n",
        "\n",
        "hub_path = 'gs://simclr-checkpoints/simclrv2/finetuned_100pct/r50_1x_sk0/hub/'\n",
        "module = hub.Module(hub_path, trainable=False)\n",
        "key = module(inputs=x['image'], signature=\"default\", as_dict=True)\n",
        "\n",
        "\n",
        "with tf.variable_scope('head_supervised_new', reuse=tf.AUTO_REUSE):\n",
        "  logits_t = tf.layers.dense(inputs=key['final_avg_pool'], units=num_classes)\n",
        "loss_t = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    labels=tf.one_hot(x['label'], num_classes), logits=logits_t))\n",
        "\n",
        "\n",
        "optimizer = LARSOptimizer(\n",
        "    learning_rate,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    exclude_from_weight_decay=['batch_normalization', 'bias', 'head_supervised'])\n",
        "variables_to_train = tf.trainable_variables() \n",
        "train_op = optimizer.minimize(\n",
        "    loss_t, global_step=tf.train.get_or_create_global_step(),\n",
        "    var_list=variables_to_train)\n",
        "\n",
        "print('Variables to train:', variables_to_train)\n",
        "key "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-bedbc2e1f354>:13: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-bedbc2e1f354>:13: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Variables to train: [<tf.Variable 'head_supervised_new/dense/kernel:0' shape=(2048, 5) dtype=float32_ref>, <tf.Variable 'head_supervised_new/dense/bias:0' shape=(5,) dtype=float32_ref>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'block_group1': <tf.Tensor 'module_apply_default/base_model/block_group1:0' shape=(?, 56, 56, 256) dtype=float32>,\n",
              " 'block_group2': <tf.Tensor 'module_apply_default/base_model/block_group2:0' shape=(?, 28, 28, 512) dtype=float32>,\n",
              " 'block_group3': <tf.Tensor 'module_apply_default/base_model/block_group3:0' shape=(?, 14, 14, 1024) dtype=float32>,\n",
              " 'block_group4': <tf.Tensor 'module_apply_default/base_model/block_group4:0' shape=(?, 7, 7, 2048) dtype=float32>,\n",
              " 'default': <tf.Tensor 'module_apply_default/base_model/final_avg_pool:0' shape=(?, 2048) dtype=float32>,\n",
              " 'final_avg_pool': <tf.Tensor 'module_apply_default/base_model/final_avg_pool:0' shape=(?, 2048) dtype=float32>,\n",
              " 'initial_conv': <tf.Tensor 'module_apply_default/base_model/initial_conv:0' shape=(?, 112, 112, 64) dtype=float32>,\n",
              " 'initial_max_pool': <tf.Tensor 'module_apply_default/base_model/initial_max_pool:0' shape=(?, 56, 56, 64) dtype=float32>,\n",
              " 'logits_sup': <tf.Tensor 'module_apply_default/head_supervised/linear_layer/linear_layer_out:0' shape=(?, 1000) dtype=float32>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JYvhrH5L-AT"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiiN-vQPMB0l",
        "outputId": "4f5619f0-b933-4caf-c22d-a85a88d8a2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "total_iterations = 10\n",
        "\n",
        "for it in range(total_iterations):\n",
        "  _, loss, image, logits, labels = sess.run((train_op, loss_t, x['image'], logits_t, x['label']))\n",
        "  pred = logits.argmax(-1)\n",
        "  correct = np.sum(pred == labels)\n",
        "  total = labels.size\n",
        "  print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, loss, correct/float(total)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 1] Loss: 1.7697997093200684 Top 1: 0.328125\n",
            "[Iter 2] Loss: 1.7605067491531372 Top 1: 0.296875\n",
            "[Iter 3] Loss: 1.7360889911651611 Top 1: 0.53125\n",
            "[Iter 4] Loss: 0.44026023149490356 Top 1: 0.828125\n",
            "[Iter 5] Loss: 1.0488773584365845 Top 1: 0.71875\n",
            "[Iter 6] Loss: 0.6889563798904419 Top 1: 0.84375\n",
            "[Iter 7] Loss: 0.4827876091003418 Top 1: 0.828125\n",
            "[Iter 8] Loss: 0.4045749008655548 Top 1: 0.890625\n",
            "[Iter 9] Loss: 0.3832011818885803 Top 1: 0.859375\n",
            "[Iter 10] Loss: 0.2586492896080017 Top 1: 0.921875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLjxL1W-izq"
      },
      "source": [
        "### Here we see with simclr, accuracy jumped from 74% to 92% after 10 iterations."
      ]
    }
  ]
}